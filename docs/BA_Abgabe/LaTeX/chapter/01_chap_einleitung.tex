%
% ****
\chapter{Einleitung}
\label{chap:einleitung}
% ****
%

	Diese Arbeit beschäftigt sich mit Bereichen des Reinforcement Learning und der Anwendung biologischer neuronaler Netze auf Probleme der Regelungstechnik.\\
	Hierzu wird als Basis ein Paper von Lechner et al. \glqq Worm-level Control through Search-based Reinforcement Learning\grqq{} \cite{WormLevelRL} herangezogen. Anders als herkömmliche neuronale Netze, welche meist künstlich erzeugt und angewendet werden, findet in dieser Arbeit ein biologisches Netz des Wurms C. Elegans Anwendung. Dieses Netz wurde schon durch verschiedene Paper \cite{CElegans} \cite{SimCE} \cite{Wicks1996} untersucht und vorgestellt. Um dieses Netz zu simulieren, wird auf das Leaky Integrate and Fire Modell verwiesen, welches eine gute Berechnungsgrundlage für Prozesse innerhalb biologischer neuronaler Netze bietet. Zur Berechnung der neuronalen Dynamik müssen charakteristische Größen innerhalb des Netzes berechnet werden:
	\begin{itemize}
		\item Membranpotential $U_i$ einer Nervenzelle und
		\item Anliegende Ströme $I_i$ aus Sensorneuronen, Synapsen und Gap-Junctions.
	\end{itemize}
	Um die Informationsverarbeitung innerhalb des Netzes zu nutzen, werden s.g. Sensor- und Motor-Neuronen definiert. Sensor-Neuronen nehmen Größen aus der gegebenen Umwelt auf und übersetzen diese in ein verständliches Format. In diesem Fall werden Größen auf einen Aktionsbereich von $A \in [-70mV, -20mV]$ übersetzt. Gleichzeitig können Motor-Neuronen den genannten Aktionsbereich $A$ bspw. auf translatorische Größen anwenden.\\
	Aufgrund des komplexen Berechenbarkeitsmodells biologischer neuronaler Netze müssen für alle eingesetzten Nervenzellen, Synapsen und Gap-Junctions Parameter gefunden werden, welche eine gute und stabile Simulation der gegebenen Umwelt gewährleisten. Hier wird auf die Methode des Reinforcement Learning verwiesen. Durch zufälliges Sampling der geforderten Größen in einem vorher festgelegten Bereich wird eine Vielzahl an Simulationen angesetzt. Ein Belohnungssystem gibt Aufschluss über die Güte der eingesetzten Parameter. Besonders gute Parametersätze werden nach Ablauf der Simulationszeit gespeichert. Durch eine nachgelagerte Optimierung des Netzwerkes mit Gewichtung der Synapsen und Gap-Junctions werden stabile und reproduzierbare Simulationsergebnisse erzielt. Als Simulationsumgebung wird das inverse Pendel genutzt. Dieses wird aus dem bereits bestehenden Framework OpenAI Gym importiert und integriert (\texttt{CartPole\_v0}). Das inverse Pendel kann im Lernprozess stabilisiert werden, entsprechende Parameter sowie Animationen sind in Anhang \ref{app:parameter} zu finden.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "main"
%%% End: 
