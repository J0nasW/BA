%
% ****
\chapter{Einleitung}
\label{chap:einleitung}
% ****
%

	Diese Arbeit beschäftigt sich mit Bereichen des Reinforcement Learning und der Anwendung biologischer neuronaler Netze auf Probleme der Regelungstechnik. 
	
	Hierzu wird als Basis die Arbeit von Lechner et al. \glqq Worm-level Control through Search-based Reinforcement Learning\grqq{} \cite{WormLevelRL} und \glqq Neuronal Circuit Policies\grqq{} \cite{NeuralPolicies} herangezogen. Anders als herkömmliche neuronale Netze, welche meist künstlich erzeugt und angewendet werden, findet in dieser Arbeit ein biologisches Netz des Wurms \textit{C. Elegans} Anwendung. Dieses Netz wurde bereits durch verschiedene Artikel \cite{CElegans} \cite{SimCE} \cite{Wicks1996} untersucht und vorgestellt. Um es zu simulieren, wird auf das \textit{Leaky Integrate and Fire (LIF)} -  Modell verwiesen, welches eine gute Berechnungsgrundlage für Prozesse innerhalb biologischer neuronaler Netze bietet. Zur Berechnung der neuronalen Dynamik müssen charakteristische Größen innerhalb des Netzes berechnet werden:
	\begin{itemize}
		\item Membranpotential $U_i$ einer Nervenzelle und
		\item Anliegende Ströme $I_i$ aus Sensorneuronen, Synapsen und Gap-Junctions.
	\end{itemize}
	Um die Informationsverarbeitung innerhalb des Netzes zu nutzen, werden sog. Sensor- und Motor-Neuronen definiert. Sensor-Neuronen nehmen Größen aus der gegebenen Umwelt auf und übersetzen diese in ein verständliches Format. In diesem Fall werden Größen auf einen Aktionsbereich von $A \in [-70\text{ mV}, -20\text{ mV}]$ übersetzt. Gleichzeitig können Motor-Neuronen den genannten Aktionsbereich $A$ bspw. auf translatorische Größen anwenden.
	
	Aufgrund des komplexen Berechenbarkeitsmodells biologischer neuronaler Netze müssen für alle eingesetzten Nervenzellen, Synapsen und Gap-Junctions Parameter gefunden werden, welche eine gute und stabile Simulation der gegebenen Umwelt gewährleisten. Hier wird auf die Methode des \textit{Reinforcement Learning} verwiesen. Durch Generierung von zufälligen Parametern anhand einer Gleichverteilung mit gegebenen Grenzen wird eine Vielzahl an Simulationen ausgeführt. Ein Belohnungssystem gibt Aufschluss über die Güte der eingesetzten Parameter. Besonders gute Parametersätze werden nach Ablauf der Simulationszeit gespeichert. Im weiteren Verlauf wird auch auf die Methode der genetischen Algorithmen hingewiesen, welche einen evolutionären Ansatz verfolgt und die Grenzen der genannten Gleichverteilung für Parameter zielgerichtet einschränken kann.
	Durch eine nachgelagerte Optimierung des Netzwerkes mit Gewichtung der Synapsen und Gap-Junctions werden stabile und reproduzierbare Simulationsergebnisse erzielt.
	
	Als Simulationsumgebung wird das inverse Pendel genutzt. Das sog. \texttt{CartPole\_v0} wird aus dem bereits bestehenden Framework OpenAI Gym \cite{Brockman2016} importiert. Das inverse Pendel kann im Lernprozess stabilisiert werden, entsprechende Parameter sowie Animationen sind in Anhang \ref{app:datenblatt} und \ref{app:parameter} zu finden.
	
	Letztlich wird eine Zusammenfassung sowie ein ausführlicher Ausblick über die Ergebnisse dieser Arbeit präsentiert.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "main"
%%% End: 
